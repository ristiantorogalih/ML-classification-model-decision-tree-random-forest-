library(caret)
confusionMatrix(data = pred_test, reference = test$party, positive = "republican")
(150/400*100/400*400/500)/((150/400*100/400*400/500) + (100/500*20/100*15/100))
# P(Positive|Good & Catchy) if Good = 0
(150/400*100/400*400/500)/((150/400*100/400*400/500) + (100/500*20/100*0/100))
# add laplace = 1
model_party_laplace <- naiveBayes(party ~., data = train, laplace = 1)
sms <- read.csv("data_input/spam.csv", stringsAsFactors = FALSE, encoding = "UTF-8")
library(dplyr)
library(stringr)
sms <- sms %>%
select(label = v1, text = v2) %>%
mutate(label = as.factor(label),
text = str_to_lower(text))
prop.table(table(sms$label))
library(tm)
# convert text to corpus
sms.corpus <- VCorpus(VectorSource(sms$text))
# check one of the texts
sms.corpus[[10]]$content
# check the class of sms.corpus
class(sms.corpus)
sms.corpus <- tm_map(sms.corpus, FUN = removeNumbers)
sms.corpus[[10]]$content
# stopwords("en")
stopwords("en")
stopwords_id <- read.delim("stopwords-id.txt", header = 0)
sms.corpus <- tm_map(sms.corpus, FUN = removeWords, stopwords("en"))
sms.corpus[[10]]$content
stopwords_list <- c("a", "b", "c")
sms.corpus <- tm_map(sms.corpus, FUN = removePunctuation)
sms.corpus[[10]]$content
# see how stemming works
library(SnowballC)
# stem words with `stemDocument`
sms.corpus <- tm_map(sms.corpus, stemDocument)
sms.corpus[[10]]$content
str_replace_all(string = "@fafilia makan siang yuk", pattern = "@", replacement = "")
library(textstem)
kamus_malang <- data.frame(kata = c("kera", "ngalam"), c("arek", "malang"))
lemmatize_strings(x = c("arek ngalam", "'cause updated PC"),
dictionary = kamus_malang)
lexicon::hash_internet_slang
sms.corpus <- tm_map(sms.corpus, stripWhitespace)
sms.corpus[[10]]$content
# Examine our dtm
sms.dtm <- DocumentTermMatrix(sms.corpus)
inspect(sms.dtm)
sms.corpus[[1085]]$content
RNGkind(sample.kind = "Rounding")
set.seed(123)
idx <- sample(nrow(sms.dtm), nrow(sms.dtm)*0.75)
train_sms <- sms.dtm[idx,]
test_sms <- sms.dtm[-idx,]
# Store the label in 'train_label' and 'test_label'
train_label <- sms[idx, "label"]
test_label <- sms[-idx, "label"]
# All terms that appear in at least 20 documents.
sms_freq <- findFreqTerms(sms.dtm, lowfreq = 20)
head(sms_freq) # sms freq merupakan prediktor/kata2 yang kemunculannya min 20 di data
length(sms_freq)
# Please subset the column of train-test dataset with just using column which column names are in sms_freq.
train_freq <- train_sms[, sms_freq] # subset kolom2 dari sms_freq(yg kemunculannya min 20x)
test_freq <- test_sms[,sms_freq]
inspect(train_freq)
bernoulli_conv <- function(x){
x <- as.factor(as.numeric(x > 0))
}
# Check your function here (using 'sapply')
data <- c(1,2,3,9,0,0,0)
sapply(data, bernoulli_conv)
# Margin=1 (by row), margin=2 (by column)
train_bn <- apply(train_freq, MARGIN = 2, FUN = bernoulli_conv)
test_bn <- apply(test_freq, MARGIN = 2, FUN = bernoulli_conv)
# Create your model here.
model_sms <- naiveBayes(x = train_bn, y = train_label, laplace = 1)
# predict
pred_test <- predict(model_sms, newdata = test_bn)
confusionMatrix(pred_test, reference = test_label, positive = "spam")
nrow(test_bn)
length(test_label)
contoh<-predict(model_sms, newdata=test_bn, type="raw")
contoh<-predict(model_sms, newdata=tail(test_bn), type="class")
ifelse(test=tail(contoh[,2])>0.3, yes="spam", no="ham")
contoh
contoh
contoh<-predict(model_sms, newdata=test_bn, type="raw")
#contoh<-predict(model_sms, newdata=tail(test_bn), type="class")
ifelse(test=tail(contoh[,2])>0.3, yes="spam", no="ham")
pred_prob<-predict(model_sms, newdata=test_bn, type="raw")
library(ROCR)
prediction(predictions = pred_prob, labels=as.numeric(ifelse(test_label=="spam", 1, 0)))
library(ROCR)
pred_rocr<-prediction(predictions = pred_prob[,2], labels=as.numeric(ifelse(test_label=="spam", 1, 0)))
pred_prob
perf<-performance(prediction.obj = pred_rocr, measure = "tpr", x.measure = "fpr")
library(ROCR)
# roc plot
plot(perf)
# auc
auc<-performance(pred_rocr,"auc")
auc@y.value
library(ROCR)
# roc plot
plot(perf)
# auc
auc <- performance(pred_rocr, "auc")
auc@y.values
pred_prob<-predict(model_sms, newdata=test_bn, type="raw")
iris
set.seed(100)
idx <- sample(nrow(iris), nrow(iris)*0.75)
train_iris <- iris[idx,]
test_iris <- iris[-idx,]
library(partykit)
model_iris<-ctree(species~., data=train_iris)
plot(model_iris, type="simple")
model_iris<-ctree(Species~., data=train_iris)
plot(model_iris, type="simple")
model_iris
pred_iris<-predict(model_iris, newdata = test_iris)
head(pred_iris)
confusionMatrix(pred_iris, reference = test_iris$Species)
diabetes <- read.csv("data_input/diabetes.csv")
diabetes
diab <- diabetes %>%
mutate(diabetes = as.factor(diabetes))
library(rsample)
set.seed(100)
idx <- initial_split(diab, prop = 0.9, strata = "diabetes")
train <- training(idx)
test <- testing(idx)
prop.table(table(train$diabetes))
model_diabet<-ctree(diabetes~., data=train)
plot(model_diabet, type="simple")
# hasil prediksi berupa kelas
pred_diabet<-predict(model_diabet, newdata = test)
head(pred_diabet)
# hasil prediksi berupa kelas
pred_diabet<-predict(model_diabet, newdata = test, type="prob")
head(pred_diabet)
pred_diabet<-predict(model_diabet, newdata = test)
head(pred_diabet)
# hasil prediksi berupa peluang
pred_diabet_prob<-predict(model_diabet, newdata = test, type="prob")
head(pred_diabet_prob)
library(caret)
confusionMatrix(pred_diabet, reference = test$diabetes)
model_diabet
confusionMatrix(pred_diabet, reference = test$diabetes, positive = "pos")
confusionMatrix(pred_diabet, reference = test$diabetes)
confusionMatrix(pred_diabet, reference = test$diabetes, positive="pos")
plot(model_diabet, type="simple")
# predict data test
pred_diabet<-predict(model_diabet, newdata = test, type="response")
#predict data train
pred_diabet_train<-predict(model_diabet, newdata = train, type="response")
confusionMatrix(pred_diabet, reference = test$diabetes, positive="pos")
confusionMatrix(pred_diabet_train, reference = train$diabetes, positive="pos")
plot(model_diabet)
model_tune<-ctree(diabetes~.,data=train,
control = ctree_control(mincriterion = 0.8)
)
plot(model_tune)
model_tune
model_diabet
plot(model_diabet)
plot(model_tune)
model_tune<-ctree(diabetes~.,data=train,
control = ctree_control(mincriterion = 0.8, minsplit = 50)
)
plot(model_tune)
model_tune<-ctree(diabetes~.,data=train,
control = ctree_control(mincriterion = 0.8, minsplit = 50, minbucket = 21)
)
plot(model_tune)
model_tune<-ctree(diabetes~.,data=train,
control = ctree_control(mincriterion = 0.8, minsplit = 50, minbucket = 21)
)
plot(model_tune)
confusionMatrix(model_tune, reference = train$diabetes, positive="pos")
model_tune<-ctree(diabetes~.,data=train,
control = ctree_control(mincriterion = 0.8, minsplit = 50, minbucket = 21)
)
plot(model_tune)
pred_diabet_tune_test<-predict(model_tune, newdata = test, type="response")
confusionMatrix(pred_diabet_tune_test, reference = test$diabetes, positive="pos")
pred_diabet_tune_train<-predict(model_tune, newdata = train, type="response")
confusionMatrix(pred_diabet_tune_train, reference = train$diabetes, positive="pos")
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 99)
library(tidyverse)
library(tm)
tinder <- read.csv("data_input/sample_tinder.csv")
tinder
head(tinder)
prop.table(table(tinder$tinder))
# proportion of an Indonesian adult is using Tinder
225/(1513+225)
p_head <- 1/2
p_ace <- 4/52
p_head*p_ace
head(tinder)
# Find the probability of someone's using tinder when their age falls in 18-29 age group
# Notation: P(Tinder=User | Age="18-29")
tinder %>%
filter(tinder == "No") %>%  # subset rows
select(agegroup) %>% # subset kolom
table() %>%
prop.table()
prop.table(table(tinder$agegroup))
prop.table(table(tinder$tinder))
(0.26666667*0.1294591)/((0.26666667*0.1294591)+ (0.1685393*0.8705409))
library(e1071)
# Data preprocessing
tinder <- tinder %>%
mutate(tinder = as.factor(tinder),
agegroup = as.factor(agegroup),
gender = as.factor(gender))
# create model
model_tinder <- naiveBayes(tinder~., data = tinder)
model_tinder
# predict
# untuk menghasilkan peluangnya
predict(model_tinder,
newdata = data.frame(agegroup = "18-29", gender = "F"),
type = "raw")
# untuk menghasilkan output kategorinya
predict(model_tinder,
newdata = data.frame(agegroup = "18-29", gender = "F"))
(0.26666667 * 0.6711111 * 0.1294591) /((0.26666667 * 0.6711111 * 0.1294591) + (0.16853933 * 0.5829478 * 0.8705409))
votes <- read.csv("data_input/votes.txt")
names(votes) <- c("party",
"hcapped_infants",
"watercost_sharing",
"adoption_budget_reso",
"physfee_freeze",
"elsalvador_aid",
"religious_grps",
"antisatellite_ban",
"nicaraguan_contras",
"mxmissile",
"immigration",
"synfuels_cutback",
"education_funding",
"superfundright_sue",
"crime",
"dutyfree_exps",
"expadmin_southafr"
)
glimpse(votes)
str(votes)
votes <- votes %>%
mutate_if(is.character, as.factor)
prop.table(table(votes$party))
votes %>%
pivot_longer(-party, names_to = "policy", values_to = "response") %>%
group_by(policy, party, response) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(response = forcats::fct_relevel(response, c("n","?","y"))) %>%
ggplot(aes(policy, n))+
coord_flip()+
facet_wrap(~party)+
geom_col(aes(fill = response), position = "fill")+
geom_hline(yintercept = .5, linetype = "dashed")+
labs(title = "How Democrat vs. Republican Voted", x = NULL, y = NULL, fill = NULL)+
theme(legend.position = "top",
plot.title = element_text(hjust = .5))
RNGkind(sample.kind = "Rounding")
set.seed(123)
index <- sample(nrow(votes), nrow(votes)*0.8)
train <- votes[index,]
test <- votes[-index,]
prop.table(table(train$party))
# model
model_votes <- naiveBayes(party~., data = train)
# predict
pred_test <- predict(model_votes, newdata = test, type = "class")
library(caret)
confusionMatrix(data = pred_test, reference = test$party, positive = "republican")
(150/400*100/400*400/500)/((150/400*100/400*400/500) + (100/500*20/100*15/100))
# P(Positive|Good & Catchy) if Good = 0
(150/400*100/400*400/500)/((150/400*100/400*400/500) + (100/500*20/100*0/100))
# add laplace = 1
model_party_laplace <- naiveBayes(party ~., data = train, laplace = 1)
sms <- read.csv("data_input/spam.csv", stringsAsFactors = FALSE, encoding = "UTF-8")
library(dplyr)
library(stringr)
sms <- sms %>%
select(label = v1, text = v2) %>%
mutate(label = as.factor(label),
text = str_to_lower(text))
prop.table(table(sms$label))
library(tm)
# convert text to corpus
sms.corpus <- VCorpus(VectorSource(sms$text))
# check one of the texts
sms.corpus[[10]]$content
# check the class of sms.corpus
class(sms.corpus)
sms.corpus <- tm_map(sms.corpus, FUN = removeNumbers)
sms.corpus[[10]]$content
# stopwords("en")
stopwords("en")
stopwords_id <- read.delim("stopwords-id.txt", header = 0)
sms.corpus <- tm_map(sms.corpus, FUN = removeWords, stopwords("en"))
sms.corpus[[10]]$content
tm_map(sms.corpus, FUN = removeWords, stopwords("en"))
stopwords_list <- c("a", "b", "c")
sms.corpus <- tm_map(sms.corpus, FUN = removePunctuation)
sms.corpus[[10]]$content
# see how stemming works
library(SnowballC)
# stem words with `stemDocument`
sms.corpus <- tm_map(sms.corpus, stemDocument)
sms.corpus[[10]]$content
str_replace_all(string = "@fafilia makan siang yuk", pattern = "@", replacement = "")
library(textstem)
lemmatize_strings(x = c("arek ngalam", "'cause updated PC"),
dictionary = kamus_malang)
kamus_malang <- data.frame(kata = c("kera", "ngalam"), c("arek", "malang"))
lexicon::hash_internet_slang
sms.corpus <- tm_map(sms.corpus, stripWhitespace)
sms.corpus[[10]]$content
# Examine our dtm
sms.dtm <- DocumentTermMatrix(sms.corpus)
inspect(sms.dtm)
sms.corpus[[1085]]$content
RNGkind(sample.kind = "Rounding")
set.seed(123)
idx <- sample(nrow(sms.dtm), nrow(sms.dtm)*0.75)
train_sms <- sms.dtm[idx,]
test_sms <- sms.dtm[-idx,]
# Store the label in 'train_label' and 'test_label'
train_label <- sms[idx, "label"]
test_label <- sms[-idx, "label"]
# All terms that appear in at least 20 documents.
sms_freq <- findFreqTerms(sms.dtm, lowfreq = 20)
head(sms_freq) # sms freq merupakan prediktor/kata2 yang kemunculannya min 20 di data
length(sms_freq)
# Please subset the column of train-test dataset with just using column which column names are in sms_freq.
train_freq <- train_sms[, sms_freq] # subset kolom2 dari sms_freq(yg kemunculannya min 20x)
test_freq <- test_sms[,sms_freq]
inspect(train_freq)
bernoulli_conv <- function(x){
x <- as.factor(as.numeric(x > 0))
}
# Check your function here (using 'sapply')
data <- c(1,2,3,9,0,0,0)
sapply(data, bernoulli_conv)
# Margin=1 (by row), margin=2 (by column)
train_bn <- apply(train_freq, MARGIN = 2, FUN = bernoulli_conv)
test_bn <- apply(test_freq, MARGIN = 2, FUN = bernoulli_conv)
# Create your model here.
model_sms <- naiveBayes(x = train_bn, y = train_label, laplace = 1)
# predict
pred_test <- predict(model_sms, newdata = test_bn)
confusionMatrix(pred_test, reference = test_label, positive = "spam")
nrow(test_bn)
length(test_label)
contoh <- predict(model_sms, newdata = test_bn, type = "raw")
head(contoh)
# threshold default 0.5
predict(model_sms, newdata = tail(test_bn), type = "raw")
# threshold diadjust tidak 0.5
ifelse(test = tail(contoh[,2]) > 0.3, yes = "spam", no = "ham")
pred_prob <- predict(object = model_sms, newdata = test_bn, type = "raw")
library(ROCR)
pred_rocr <- prediction(predictions = pred_prob[,2],
labels = as.numeric(ifelse(test_label == "spam", 1, 0)))
perf <- performance(prediction.obj = pred_rocr, measure = "tpr", x.measure = "fpr")
library(ROCR)
# roc plot
plot(perf)
# auc
auc <- performance(pred_rocr, "auc")
auc@y.values[[1]] # [[1]] untuk mengakses value dr list pertama
iris
set.seed(100)
idx <- sample(nrow(iris), nrow(iris)*0.75)
train_iris <- iris[idx,]
test_iris <- iris[-idx,]
library(partykit)
model_iris <- ctree(formula = Species ~., data = train_iris)
model_iris
plot(model_iris, type = "simple")
pred_iris <- predict(model_iris, newdata = test_iris)
head(pred_iris)
confusionMatrix(pred_iris, reference = test_iris$Species)
curve(-x*log2(x) - (1-x)*log2(1-x), xlab="proportion of one class", ylab="Entropy", lwd=3)
diabetes <- read.csv("data_input/diabetes.csv")
diabetes
diab <- diabetes %>%
mutate(diabetes = as.factor(diabetes))
library(rsample)
set.seed(100)
idx <- initial_split(diab, prop = 0.9, strata = "diabetes")
train <- training(idx)
test <- testing(idx)
prop.table(table(train$diabetes))
model_diab <- ctree(diabetes ~., data = train)
plot(model_diab, type = "simple")
# hasil prediksi berupa kelas
pred_class <- predict(model_diab, newdata = test, type = "response")
head(pred_class)
# hasil prediksi berupa peluang
pred_peluang <- predict(model_diab, newdata = test, type = "prob")
head(pred_peluang)
library(caret)
confusionMatrix(pred_class, reference = test$diabetes, positive = "pos")
# predict ke data test
pred_class <- predict(model_diab, newdata = test, type = "response")
# predict ke data train
pred_class_train <- predict(model_diab, newdata = train, type = "response")
confusionMatrix(pred_class, reference = test$diabetes, positive = "pos")
confusionMatrix(pred_class_train, reference = train$diabetes, positive = "pos")
plot(model_diab)
model_tune <- ctree(diabetes ~.,
data = train,
control = ctree_control(mincriterion = 0.99,
minsplit = 50,
minbucket = 21))
plot(model_tune)
p_test <- predict(model_tune, newdata = test)
p_train <- predict(model_tune, newdata = train)
# confusion matrix
confusionMatrix(p_test, reference = test$diabetes, positive = "pos")
confusionMatrix(p_train, reference = train$diabetes, positive = "pos")
str_replace_all(string = "@fafilia makan siang yuk", pattern = "@", replacement = "")
library(textstem)
kamus_malang <- data.frame(kata = c("kera", "ngalam"), c("arek", "malang"))
lemmatize_strings(x = c("arek ngalam", "'cause updated PC"),
dictionary = kamus_malang)
lexicon::hash_internet_slang
fb <- read.csv("data_input/fitbit.csv", stringsAsFactors = F)
library(partykit)
model_iris <- ctree(formula = Species ~., data = train_iris)
model_iris
fb <- fb %>%
mutate(classe = as.factor(classe),
user_name = as.factor(user_name),
new_window = as.factor(new_window))
fb
fb[,150:158]
levels(fb$classe)
library(caret)
n0_var <- nearZeroVar(fb)
fb <- fb[,-n0_var]
library(caret)
n0_var <- nearZeroVar(fb)
fb <- fb[,-n0_var]
library(rsample)
set.seed(100)
splitted <- initial_split(data = fb, prop = 0.75, strata = "classe")
fb <- read.csv("data_input/fitbit.csv", stringsAsFactors = F)
fb <- fb %>%
mutate(classe = as.factor(classe),
user_name = as.factor(user_name),
new_window = as.factor(new_window))
fb[,150:158]
levels(fb$classe)
library(caret)
n0_var <- nearZeroVar(fb)
fb <- fb[,-n0_var]
library(rsample)
set.seed(100)
splitted <- initial_split(data = fb, prop = 0.75, strata = "classe")
train <- training(splitted)
test <- testing(splitted)
prop.table(table(train$classe))
library(animation)
ani.options(interval = 1, nmax = 15)
cv.ani(main = "Demonstration of the k-fold Cross Validation",
bty = "l")
set.seed(417)
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
fb_forest <- readRDS("fb_forest.RDS")
# print output
fb_forest
# model final yang digunakan (karena menggunakan k-fold cross validation)
fb_forest$finalModel
# print output
fb_forest
# visualize model
plot(fb_forest)
# prediktor yang dianggap penting (paling informatif untuk memprediksi target)
varimp(fb_forest)
# prediktor yang dianggap penting (paling informatif untuk memprediksi target)
varImp(fb_forest)
# prediktor yang dianggap penting (paling informatif untuk memprediksi target)
varImp(fb_forest)
# print output
fb_forest
saveRDS(fb_forest, file = "fb_forest_save.RDS")
plot(fb_forest$finalModel)
legend("topright", colnames(fb_forest$finalModel$err.rate),
col=1:6,cex=0.8,fill=1:6)
fb_forest$finalModel
pred_rf<-predict(object=fb_forest,  newdata=test)
head(pred_rf)
confusionMatrix(data=pred_rf, reference=test$classe)
head(sms)
set.seed(100)
splitted <- initial_split(data = sms, prop = 0.75, strata = "label")
train <- training(splitted)
test <- testing(splitted)
prop.table(table(train$label))
library(caret)
train_up <- upSample(x = train[, -1], y = train$label, yname = "label")
train_up
table(train_up$label)
